{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Model Experiment: Clean vs Unfiltered Data\n",
    "\n",
    "**Goal:** Train two reward models on HH-RLHF preference data — one on quality-filtered (clean) data, one on unfiltered data — and compare accuracy on a held-out test set.\n",
    "\n",
    "**Setup:** Free T4 GPU on Google Colab. ~25 minutes total runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch tqdm matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reward Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Bradley-Terry reward model: backbone -> [CLS] -> Linear -> scalar.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"distilbert-base-uncased\") -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        self.reward_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        reward = self.reward_head(cls_embedding).squeeze(-1)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conversation Parsing & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SPEAKER_RE = re.compile(r\"^(Human|Assistant):\\s*\", re.MULTILINE)\n",
    "\n",
    "\n",
    "def last_turn(text: str, role: str) -> str:\n",
    "    \"\"\"Extract the last turn for a given role from an HH-RLHF conversation.\"\"\"\n",
    "    parts = _SPEAKER_RE.split(text)\n",
    "    result = \"\"\n",
    "    for i in range(1, len(parts), 2):\n",
    "        if parts[i].lower() == role and i + 1 < len(parts):\n",
    "            result = parts[i + 1].strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "class PreferencePairDataset(Dataset):\n",
    "    \"\"\"Dataset of (prompt, chosen, rejected) tokenized preference pairs.\"\"\"\n",
    "\n",
    "    def __init__(self, prompts, chosen_responses, rejected_responses, tokenizer, max_length=256):\n",
    "        self.prompts = prompts\n",
    "        self.chosen_responses = chosen_responses\n",
    "        self.rejected_responses = rejected_responses\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.prompts[idx]\n",
    "        chosen_enc = self.tokenizer(\n",
    "            prompt, self.chosen_responses[idx],\n",
    "            truncation=True, max_length=self.max_length,\n",
    "            padding=\"max_length\", return_tensors=\"pt\",\n",
    "        )\n",
    "        rejected_enc = self.tokenizer(\n",
    "            prompt, self.rejected_responses[idx],\n",
    "            truncation=True, max_length=self.max_length,\n",
    "            padding=\"max_length\", return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_enc[\"input_ids\"].squeeze(0),\n",
    "            \"chosen_attention_mask\": chosen_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"rejected_input_ids\": rejected_enc[\"input_ids\"].squeeze(0),\n",
    "            \"rejected_attention_mask\": rejected_enc[\"attention_mask\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load HH-RLHF & Parse Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading HH-RLHF dataset...\")\n",
    "hh_train = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
    "hh_test = load_dataset(\"Anthropic/hh-rlhf\", split=\"test\")\n",
    "print(f\"Train: {len(hh_train)} examples, Test: {len(hh_test)} examples\")\n",
    "\n",
    "def parse_split(ds):\n",
    "    \"\"\"Parse a HuggingFace split into (prompts, chosen, rejected) lists.\"\"\"\n",
    "    prompts, chosen, rejected = [], [], []\n",
    "    for row in ds:\n",
    "        p = last_turn(row[\"chosen\"], \"human\")\n",
    "        c = last_turn(row[\"chosen\"], \"assistant\")\n",
    "        r = last_turn(row[\"rejected\"], \"assistant\")\n",
    "        if p and c and r:\n",
    "            prompts.append(p)\n",
    "            chosen.append(c)\n",
    "            rejected.append(r)\n",
    "    return prompts, chosen, rejected\n",
    "\n",
    "# Parse all conversations\n",
    "print(\"Parsing train split...\")\n",
    "all_prompts, all_chosen, all_rejected = parse_split(hh_train)\n",
    "print(f\"Parsed {len(all_prompts)} valid train examples\")\n",
    "\n",
    "print(\"Parsing test split...\")\n",
    "test_prompts, test_chosen, test_rejected = parse_split(hh_test)\n",
    "print(f\"Parsed {len(test_prompts)} valid test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload flagged_indices.json & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload flagged_indices.json (exported from local PostgreSQL)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Select flagged_indices.json\n",
    "\n",
    "with open(\"flagged_indices.json\") as f:\n",
    "    flagged_indices = set(json.load(f))\n",
    "\n",
    "print(f\"Loaded {len(flagged_indices)} flagged indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index mapping: we need to know which parsed examples correspond to\n",
    "# which original dataset indices (some get skipped if parsing fails)\n",
    "valid_to_original = []  # valid_idx -> original dataset index\n",
    "for i, row in enumerate(hh_train):\n",
    "    p = last_turn(row[\"chosen\"], \"human\")\n",
    "    c = last_turn(row[\"chosen\"], \"assistant\")\n",
    "    r = last_turn(row[\"rejected\"], \"assistant\")\n",
    "    if p and c and r:\n",
    "        valid_to_original.append(i)\n",
    "\n",
    "# Split into clean vs unfiltered\n",
    "clean_prompts, clean_chosen, clean_rejected = [], [], []\n",
    "for valid_idx in range(len(all_prompts)):\n",
    "    original_idx = valid_to_original[valid_idx]\n",
    "    if original_idx not in flagged_indices:\n",
    "        clean_prompts.append(all_prompts[valid_idx])\n",
    "        clean_chosen.append(all_chosen[valid_idx])\n",
    "        clean_rejected.append(all_rejected[valid_idx])\n",
    "\n",
    "print(f\"Unfiltered: {len(all_prompts)} examples\")\n",
    "print(f\"Clean (filtered): {len(clean_prompts)} examples\")\n",
    "print(f\"Removed: {len(all_prompts) - len(clean_prompts)} flagged examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-5\n",
    "EPOCHS = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "clean_dataset = PreferencePairDataset(clean_prompts, clean_chosen, clean_rejected, tokenizer, MAX_LENGTH)\n",
    "unfiltered_dataset = PreferencePairDataset(all_prompts, all_chosen, all_rejected, tokenizer, MAX_LENGTH)\n",
    "test_dataset = PreferencePairDataset(test_prompts, test_chosen, test_rejected, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"Clean dataset: {len(clean_dataset)} pairs\")\n",
    "print(f\"Unfiltered dataset: {len(unfiltered_dataset)} pairs\")\n",
    "print(f\"Test dataset: {len(test_dataset)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward_model(model, dataset, epochs=1, batch_size=8, lr=2e-5, device=\"cuda\"):\n",
    "    \"\"\"Train with Bradley-Terry pairwise loss.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {\"epoch_loss\": [], \"epoch_accuracy\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        for batch in tqdm(loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "            chosen_ids = batch[\"chosen_input_ids\"].to(device)\n",
    "            chosen_mask = batch[\"chosen_attention_mask\"].to(device)\n",
    "            rejected_ids = batch[\"rejected_input_ids\"].to(device)\n",
    "            rejected_mask = batch[\"rejected_attention_mask\"].to(device)\n",
    "\n",
    "            reward_chosen = model(chosen_ids, chosen_mask)\n",
    "            reward_rejected = model(rejected_ids, rejected_mask)\n",
    "\n",
    "            loss = -F.logsigmoid(reward_chosen - reward_rejected).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * chosen_ids.size(0)\n",
    "            correct += (reward_chosen > reward_rejected).sum().item()\n",
    "            total += chosen_ids.size(0)\n",
    "\n",
    "        epoch_loss = total_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        history[\"epoch_loss\"].append(epoch_loss)\n",
    "        history[\"epoch_accuracy\"].append(epoch_acc)\n",
    "        print(f\"  Epoch {epoch + 1}: loss={epoch_loss:.4f}, accuracy={epoch_acc:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_reward_model(model, dataset, batch_size=8, device=\"cuda\"):\n",
    "    \"\"\"Evaluate accuracy on a preference pair dataset.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    total_gap = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            chosen_ids = batch[\"chosen_input_ids\"].to(device)\n",
    "            chosen_mask = batch[\"chosen_attention_mask\"].to(device)\n",
    "            rejected_ids = batch[\"rejected_input_ids\"].to(device)\n",
    "            rejected_mask = batch[\"rejected_attention_mask\"].to(device)\n",
    "\n",
    "            reward_chosen = model(chosen_ids, chosen_mask)\n",
    "            reward_rejected = model(rejected_ids, rejected_mask)\n",
    "\n",
    "            gap = reward_chosen - reward_rejected\n",
    "            correct += (gap > 0).sum().item()\n",
    "            total_gap += gap.sum().item()\n",
    "            total += chosen_ids.size(0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": correct / total,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"avg_reward_gap\": total_gap / total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Model A (Clean Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training Model A (CLEAN data)\")\n",
    "print(\"=\" * 60)\n",
    "model_a = RewardModel(MODEL_NAME)\n",
    "history_a = train_reward_model(model_a, clean_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Model B (Unfiltered Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training Model B (UNFILTERED data)\")\n",
    "print(\"=\" * 60)\n",
    "model_b = RewardModel(MODEL_NAME)\n",
    "history_b = train_reward_model(model_b, unfiltered_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Both Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating Model A (clean) on test set...\")\n",
    "results_a = evaluate_reward_model(model_a, test_dataset, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "print(\"\\nEvaluating Model B (unfiltered) on test set...\")\n",
    "results_b = evaluate_reward_model(model_b, test_dataset, batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS: Reward Model Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'Clean (A)':<15} {'Unfiltered (B)':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Training examples':<25} {len(clean_dataset):<15,} {len(unfiltered_dataset):<15,}\")\n",
    "print(f\"{'Train accuracy':<25} {history_a['epoch_accuracy'][-1]:<15.4f} {history_b['epoch_accuracy'][-1]:<15.4f}\")\n",
    "print(f\"{'Train loss':<25} {history_a['epoch_loss'][-1]:<15.4f} {history_b['epoch_loss'][-1]:<15.4f}\")\n",
    "print(f\"{'Test accuracy':<25} {results_a['accuracy']:<15.4f} {results_b['accuracy']:<15.4f}\")\n",
    "print(f\"{'Test correct/total':<25} {results_a['correct']}/{results_a['total']:<10} {results_b['correct']}/{results_b['total']:<10}\")\n",
    "print(f\"{'Avg reward gap':<25} {results_a['avg_reward_gap']:<15.4f} {results_b['avg_reward_gap']:<15.4f}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "diff = results_a[\"accuracy\"] - results_b[\"accuracy\"]\n",
    "winner = \"Clean (A)\" if diff > 0 else \"Unfiltered (B)\"\n",
    "print(f\"\\nAccuracy difference: {abs(diff):.4f} ({abs(diff)*100:.2f}%)\")\n",
    "print(f\"Winner: {winner}\")\n",
    "if diff > 0:\n",
    "    print(\"Data filtering IMPROVED reward model quality.\")\n",
    "else:\n",
    "    print(\"Data filtering did NOT improve reward model quality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "labels = [\"Clean (A)\", \"Unfiltered (B)\"]\n",
    "accuracies = [results_a[\"accuracy\"] * 100, results_b[\"accuracy\"] * 100]\n",
    "colors = [\"#2ecc71\", \"#e74c3c\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Test accuracy comparison\n",
    "bars = axes[0].bar(labels, accuracies, color=colors, edgecolor=\"black\", linewidth=0.5)\n",
    "axes[0].set_ylabel(\"Accuracy (%)\")\n",
    "axes[0].set_title(\"Test Accuracy: Clean vs Unfiltered\")\n",
    "axes[0].set_ylim(45, max(accuracies) + 5)\n",
    "axes[0].axhline(y=50, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Random baseline\")\n",
    "axes[0].legend()\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
    "                 f\"{acc:.2f}%\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "# Reward gap comparison\n",
    "gaps = [results_a[\"avg_reward_gap\"], results_b[\"avg_reward_gap\"]]\n",
    "bars2 = axes[1].bar(labels, gaps, color=colors, edgecolor=\"black\", linewidth=0.5)\n",
    "axes[1].set_ylabel(\"Avg Reward Gap\")\n",
    "axes[1].set_title(\"Avg Reward Gap (chosen - rejected)\")\n",
    "axes[1].axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "for bar, gap in zip(bars2, gaps):\n",
    "    axes[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "                 f\"{gap:.4f}\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reward_model_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Chart saved to reward_model_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
